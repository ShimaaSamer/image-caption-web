# -*- coding: utf-8 -*-
"""image-caption-flickr-dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VbLZ2ZEwkMIn5N_M30pi4WgmjvzORsFs

**Import Modules**
"""

import os
import pickle
import numpy as np
from tqdm.notebook import tqdm

from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical, plot_model
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add

BASE_DIR = "/kaggle/input/flickr8k"
WORKING_DIR = "/kaggle/working/"

"""## Extract Image Features"""

# Load VGG16 Model
model = VGG16()
# Restructure The Model
model = Model(inputs=model.inputs, outputs=model.layers[-2].output)
# Summerize
print(model.summary())

# Extract Feature From Image
features = {}
directory = os.path.join(BASE_DIR, 'Images')

for img_name in tqdm(os.listdir(directory)):
    # Load Image From File
    img_path = directory + '/' + img_name
    image = load_img(img_path, target_size = (224, 224))
    # Convert Image Pixels To numpy Array
    image = img_to_array(image)
    # Reshape Data For Model
    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
    # Preprocess Image For VGG
    image = preprocess_input(image)
    # Extract The Feature
    feature = model.predict(image, verbose=0)
    # Get Image ID
    image_id = img_name.split(".")[0]
    # Store Feature
    features[image_id] = feature

# Store Feture In Pickle
pickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))

# Load Features From Pickle
with open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:
    features = pickle.load(f)

"""## Load The Captions Data"""

with open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:
    next(f)
    captions_doc = f.read()

# Create Mapping Of Image To Captions
mapping = {}
# Process Lines
for line in tqdm(captions_doc.split('\n')):
    # Split The Line By Comma
    tokens = line.split(',')
    if len(line) < 2 :
        continue
    image_id, caption = tokens[0], tokens[1:]
    # Remove Extention From image_id
    image_id = image_id.split('.')[0]
    # Convert Caption List To String
    caption =  ' '.join(caption)
    # Create List If Needed
    if image_id not in mapping :
        mapping[image_id] = []
    # Store The Caption
    mapping[image_id].append(caption)

len(mapping)

def clean(mapping):
    for key, captions in mapping.items():
        for i in range(len(captions)):
            # Take One Caption At A Time
            caption = captions[i]
            # Preprocessing Steps
            # Convert To LowerCase
            caption = caption.lower()
            # Delete digits, Special Charcters, etcs.
            caption = caption.replace('[^A-Za-z]', '')
            # Delete Additional Spaces
            caption = caption.replace('\s+', ' ')
            # Add Start And End Tags To The Captions
            caption = '<start> ' + " ".join([word for word in caption.split() if len(word)>1]) + ' <end>'
            captions[i] = caption

mapping['1000268201_693b08cb0e']

# Preprocess The Text
clean(mapping)

# After Preprocess of text
mapping['1000268201_693b08cb0e']

all_caption = []
for key in mapping:
  for caption in mapping[key]:
    all_caption.append(caption)

len(all_caption)

all_caption[:10]

# Tokenize The Text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_caption)
vocab_size = len(tokenizer.word_index) + 1

vocab_size

# Get Maximum Length Of The Caption Available
max_length = max(len(caption.split()) for caption in all_caption)
max_length

"""## Train Test Split"""

image_ids = list(mapping.keys())
split = int(len(image_ids) * 0.90)
train = image_ids[:split]
test = image_ids[split:]

# <start> girl going into wooden building <end>
#         X             Y
# <start>                    girl
# <start> girl               going
# <start> girl going         into
# -------------------------------------
# <start> girl going into wooden building          <end>

# Create Data Generator To Get Data In Batch (avoids session crach)
def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):
  # Loop Over Images
  X1, X2, y = list(), list(), list()
  n = 0
  while 1:
    for key in data_keys:
      n += 1
      captions = mapping[key]
      # Loop Over Captions
      for caption in captions:
        # Encode Sequences
        seq = tokenizer.texts_to_sequences([caption])[0]
        # Split The Sequence Into X, Y Pairs
        for i in range(1, len(seq)):
          # Split Into Input And Output Pairs
          in_seq, out_seq = seq[:i], seq[i]
          # Pad Input Sequence (changed to post-padding)
          in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post')[0]
          # Encode Output Sequence
          out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]

          # Store The Sequences
          X1.append(features[key][0])
          X2.append(in_seq)
          y.append(out_seq)

      if n == batch_size:
        X1 = np.array(X1)
        X2 = np.array(X2)
        y = np.array(y)
        yield (X1, X2), y
        X1, X2, y = list(), list(), list()
        n = 0

"""## Model Creation"""

from IPython.utils.py3compat import decode
# Encoder Model
# Image Feature Layers
inputs1 = Input(shape=(4096,))
fe1 = Dropout(0.4)(inputs1)
fe2 = Dense(256, activation='relu')(fe1)
# Sequence Feature Layers
inputs2 = Input(shape=(max_length,))
se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
se2 = Dropout(0.4)(se1)
se3 = LSTM(256, use_cudnn=False)(se2) # Added use_cudnn=False

# Decoder Model
decoder1 = add([fe2, se3])
decoder2 = Dense(256, activation='relu')(decoder1)
outputs = Dense(vocab_size, activation='softmax')(decoder2)

model = Model(inputs=[inputs1, inputs2], outputs=outputs)
model.compile(loss='categorical_crossentropy', optimizer='adam')

# Plot The Model
plot_model(model, to_file='model.png', show_shapes=True)

# Train The Model
epochs = 15
batch_size = 32
steps = len(train) // batch_size

for i in range(epochs):
  # Create Data Generator
  generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)
  # Fit For One Epoch
  model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)

# save the model
model.save(WORKING_DIR+ '/best_model.h5')

"""## Generate Captions for the Image"""

def idx_to_word(integer, tokenizer):
  for word, index in tokenizer.word_index.items():
    if index == integer:
      return word
  return None

# generate caption for an image
def predict_caption(model, image, tokenizer, max_length):
  # add start tag for generation process
  in_text = '<start>'
  # iterate over the max length of sequence
  for i in range(max_length):
    # encode input sequence
    sequence = tokenizer.texts_to_sequences([in_text])[0]
    # pad the sequence
    sequence = pad_sequences([sequence], max_length)
    # predict next word
    yhat = model.predict([image, sequence], verbose=0)
    # get index with high probability
    yhat = np.argmax(yhat)
    # convert index to word
    word = idx_to_word(yhat, tokenizer)
    # stop if word not found
    if word is None:
      break
    # append word as input for generating next word
    in_text += " " + word
    # stop if we reach end tag
    if word == '<end>':
      break
  return in_text

from nltk.translate.bleu_score import corpus_bleu

# validate with test data
actual, predicted = list(), list()

for key in tqdm(test):
  # get actual caption
  captions = mapping [key]
  # predict the caption for image
  y_pred = predict_caption(model, features[key], tokenizer, max_length)
  # split into words
  actual_captions = [caption.split() for caption in captions]
  y_pred = y_pred.split()
  # append to the list
  actual.append(actual_captions)
  predicted.append(y_pred)

# calculate BLEU score
print("BLEU-1: %f" % corpus_bleu(actual, predicted, weights = (1.0, 0, 0, 0)))
print("BLEU-2: %f" % corpus_bleu(actual, predicted, weights = (0.5, 0.5, 0, 0)))

"""## Visualize the Results"""

from PIL import Image
import matplotlib.pyplot as plt
def generate_caption(image_name):
  # load the image
  # image_name = "1001773457_577c3a7d70.jpg"
  image_id = image_name.split('.')[0]
  img_path = os.path.join(BASE_DIR, "Images", image_name)
  image = Image.open(img_path)
  captions = mapping[image_id]
  print('----------------Actual----------------')
  for caption in captions:
    print(caption)
  # predict the caption
  y_pred = predict_caption(model, features[image_id], tokenizer, max_length)
  print('----------------Predicted----------------')
  print(y_pred)
  plt.imshow(image)

generate_caption("1001773457_577c3a7d70.jpg")

generate_caption("1002674143_1b742ab4b8.jpg")